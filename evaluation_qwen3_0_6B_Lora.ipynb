{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO2RyvyswtOQXeD5RC3wikr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mightyoctopus/lora-fine-tuning-qwen3-0.6b-medical-evaluation-code/blob/main/evaluation_qwen3_0_6B_Lora.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1pIp_S21v8r"
      },
      "outputs": [],
      "source": [
        "!pip install -q peft transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "ipNiXKi57b9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    default_data_collator\n",
        ")\n",
        "\n",
        "from peft import PeftModel"
      ],
      "metadata": {
        "id": "g8VTpjJv2wnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "adapter_path = \"MightyOctopus/qwen3-0.6B-lora-medical\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ").eval()\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name,trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "tmp_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "tuned_model = PeftModel.from_pretrained(tmp_model, adapter_path)\n",
        "tuned_model = tuned_model.merge_and_unload().eval()\n",
        "tuned_model.generation_config.pad_token_id = tokenizer.pad_token_type_id\n",
        "tuned_model.generation_config.eos_token_id = tokenizer.eos_token_id"
      ],
      "metadata": {
        "id": "KsZsciEN3h2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "    texts = []\n",
        "\n",
        "    for convo in batch[\"conversations\"]:\n",
        "        for turn in convo:\n",
        "            human_msg = turn[\"value\"] if turn[\"from\"] == \"human\" else \"\"\n",
        "            assisant_msg = turn[\"value\"] if turn[\"from\"] == \"gpt\" else \"\"\n",
        "\n",
        "            texts.append(f\"### Instruction:\\n{human_msg}\\n### Response:\\n{assisant_msg}\")\n",
        "\n",
        "\n",
        "    tokens = tokenizer(\n",
        "        texts,\n",
        "        padding=\"max_length\",\n",
        "        max_length=256,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].clone()\n",
        "    print(tokens[\"labels\"][tokens[\"attention_mask\"] == 0])\n",
        "    tokens[\"labels\"][tokens[\"attention_mask\"] == 0] = -100   # ignore padding tokens\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "VM8YTJfn5qqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"Rabe3/QA_Synthatic_Medical_data\"\n",
        "eval_ds = load_dataset(dataset_name, \"default\", split=\"train[90%:]\")\n",
        "eval_ds = eval_ds.map(tokenize, batched=True, remove_columns=eval_ds.column_names)\n",
        "eval_ds = eval_ds.with_format(\"torch\")"
      ],
      "metadata": {
        "id": "gkwcmUIl5wvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_loader = DataLoader(\n",
        "    eval_ds,\n",
        "    batch_size=8,\n",
        "    collate_fn= default_data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "Uxw6zK9e7SGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in eval_loader:\n",
        "    print(batch[\"input_ids\"].shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "3qmawNsn9q7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def compute_perplexity(model):\n",
        "    losses = []\n",
        "\n",
        "    for batch in eval_loader:\n",
        "        batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
        "        loss = model(**batch).loss\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    print(\"Loss Lenght: \", len(losses))\n",
        "    print(\"Sum: \", sum(losses))\n",
        "    return math.exp(sum(losses) / len(losses))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hUKThve2J9Pl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Base Model Perplexity: {compute_perplexity(base_model):.2f}\")\n",
        "print(f\"Tuned Model Perplexity: {compute_perplexity(tuned_model):.2f}\")"
      ],
      "metadata": {
        "id": "Lz4tgiiMO8fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "raw_data = load_dataset(dataset_name, \"default\", split=\"train[90%:]\")\n",
        "\n",
        "ref_questions = [convo[0][\"value\"] for convo in raw_data[\"conversations\"]]\n",
        "ref_answers = [convo[1][\"value\"] for convo in raw_data[\"conversations\"]]\n",
        "# print(ref_questions[0])\n",
        "\n",
        "\n",
        "def generate(model, instruction):\n",
        "    token_id = tokenizer(\n",
        "        f\"### Instruction:\\n{instruction}\\n### Response:\\n\",\n",
        "        return_tensors=\"pt\"\n",
        "    ).input_ids.to(\"cuda\")\n",
        "\n",
        "    # print(\"TOKEN ID\", token_id)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(token_id, max_new_tokens=256)\n",
        "\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "e9_pNZho9RQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ref_answers[0])"
      ],
      "metadata": {
        "id": "2LGxgijvUDjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(generate(base_model, ref_questions[0]))\n",
        "\n",
        "print(\"=======================================================\")\n",
        "print(\"BASE MODEL RESPONSE: \", generate(base_model, ref_questions[0]))\n",
        "print(\"=======================================================\")\n",
        "print(\"TUNED MODEL RESPONSE: \", generate(tuned_model, ref_questions[0]))"
      ],
      "metadata": {
        "id": "ZxodThT4IgMy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}